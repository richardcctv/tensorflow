{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\",one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#和整个训练相关的常量\n",
    "#学习率\n",
    "learningrate = 0.01\n",
    "#epoch\n",
    "epoch_size = 100\n",
    "#batch_size\n",
    "batch_size = 100\n",
    "#tensorboard 日志目录\n",
    "logs_path = \"logs1\"\n",
    "\n",
    "#定义多层感知机模型里的常量\n",
    "hidden_layer1 = 256\n",
    "hidden_layer2 = 256\n",
    "\n",
    "\n",
    "#数据特征\n",
    "feature_size = 784\n",
    "#类别数目\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#输入占位符\n",
    "def get_placeholder():\n",
    "    train_x = tf.placeholder(dtype=tf.float32,shape=[None,784],name=\"InputData\")\n",
    "    train_y = tf.placeholder(dtype=tf.float32,shape=[None,10],name=\"LabelData\")\n",
    "    return train_x,train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape=shape,stddev=0.05),name=\"weights\")\n",
    "\n",
    "def get_bias(shape):\n",
    "    return tf.Variable(tf.zeros(shape=shape),name = \"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(train_x):\n",
    "    with tf.name_scope(\"hidden1\"):\n",
    "        weights = get_weights(shape=[feature_size,hidden_layer1])\n",
    "        bias = get_bias(shape=[hidden_layer1])\n",
    "        layer1 = tf.nn.relu(tf.matmul(train_x,weights)+bias)\n",
    "        tf.summary.histogram(\"layer1\",layer1)\n",
    "        \n",
    "    with tf.name_scope(\"hidden2\"):\n",
    "        weights = get_weights(shape=[hidden_layer1,hidden_layer2])\n",
    "        bias = get_bias(shape=[hidden_layer2])\n",
    "        layer2 = tf.nn.relu(tf.matmul(layer1,weights)+bias)\n",
    "        tf.summary.histogram(\"layer2\",layer2)\n",
    "    \n",
    "    with tf.name_scope(\"output\"):\n",
    "        weights = get_weights(shape=[hidden_layer2,n_classes])\n",
    "        bias = get_bias(shape=[n_classes])\n",
    "        output = tf.add(tf.matmul(layer2,weights),bias)\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(output,train_y):\n",
    "    with  tf.name_scope(\"LOSS\"):\n",
    "        return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output,labels=train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1115 10:31:11.757580 16564 deprecation.py:323] From <ipython-input-7-e17b38038a20>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "W1115 10:31:12.013921 16564 deprecation.py:323] From <ipython-input-8-51995919eb44>:2: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.math.argmax` instead\n"
     ]
    }
   ],
   "source": [
    "def accuracy(output,train_y):\n",
    "    acc = tf.equal(tf.arg_max(output,1),tf.arg_max(train_y,1))\n",
    "    return tf.reduce_mean(tf.cast(acc,tf.float32))\n",
    "\n",
    "#训练\n",
    "train_x ,train_y = get_placeholder()\n",
    "\n",
    "pred = inference(train_x)\n",
    "\n",
    "cost = loss(pred,train_y)\n",
    "acc = accuracy(pred,train_y)\n",
    "\n",
    "with tf.name_scope(\"SGD\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learningrate)\n",
    "    grads = tf.gradients(cost,tf.trainable_variables())\n",
    "    grads = list(zip(grads,tf.trainable_variables()))\n",
    "    apply_grads = optimizer.apply_gradients(grads_and_vars=grads)\n",
    "\n",
    "\n",
    "#记录\n",
    "tf.summary.scalar(\"loss\",cost)\n",
    "tf.summary.scalar(\"acc\",acc)\n",
    "\n",
    "for var in tf.trainable_variables():\n",
    "    tf.summary.histogram(var.name,var)\n",
    "    \n",
    "merge_op = tf.summary.merge_all()\n",
    "\n",
    "init= tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training start...\n",
      "epoch 5,cost = 0.340684\n",
      "epoch 10,cost = 0.257436\n",
      "epoch 15,cost = 0.209423\n",
      "epoch 20,cost = 0.175721\n",
      "epoch 25,cost = 0.149715\n",
      "epoch 30,cost = 0.129819\n",
      "epoch 35,cost = 0.113661\n",
      "epoch 40,cost = 0.100585\n",
      "epoch 45,cost = 0.089468\n",
      "epoch 50,cost = 0.080150\n",
      "epoch 55,cost = 0.072292\n",
      "epoch 60,cost = 0.065295\n",
      "epoch 65,cost = 0.059209\n",
      "epoch 70,cost = 0.053744\n",
      "epoch 75,cost = 0.048900\n",
      "epoch 80,cost = 0.044794\n",
      "epoch 85,cost = 0.040907\n",
      "epoch 90,cost = 0.037401\n",
      "epoch 95,cost = 0.034292\n",
      "epoch 100,cost = 0.031454\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(\"training start...\")\n",
    "    sess.run(init)\n",
    "    summary_writer = tf.summary.FileWriter(logdir=logs_path,graph=tf.get_default_graph())\n",
    "    batch_total = int(mnist.train.num_examples/batch_size)\n",
    "    \n",
    "    for epoch in range(epoch_size):\n",
    "        \n",
    "        avg_cost = 0\n",
    "        \n",
    "        for batch in range(batch_total):\n",
    "            X,Y = mnist.train.next_batch(batch_size=batch_size)\n",
    "            \n",
    "            _,c,summary = sess.run([apply_grads,cost,merge_op],feed_dict={train_x:X,train_y:Y})\n",
    "            \n",
    "            avg_cost  += c/batch_total\n",
    "            \n",
    "            summary_writer.add_summary(summary,epoch*batch_total+batch_size)\n",
    "            \n",
    "        if (epoch+1)%5==0:\n",
    "            print(\"epoch %d,cost = %f\"%(epoch+1,avg_cost))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

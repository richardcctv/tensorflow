{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "D:\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import os\n",
    "import random\n",
    "import urllib\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.1\n",
    "batch_size = 128\n",
    "num_steps = 3000000\n",
    "display_step = 100\n",
    "eval_step = 200\n",
    "\n",
    "# Evaluation Parameters\n",
    "eval_words = [b'five',b'of', b'going', b'hardware', b'american', b'britain']\n",
    "\n",
    "# Word2Vec Parameters\n",
    "embedding_size = 200 # Dimension of the embedding vector\n",
    "max_vocabulary_size = 50000 # Total number of different words in the vocabulary\n",
    "min_occurrence = 10 # Remove all words that does not appears at least n times\n",
    "skip_window = 3 # How many words to consider left and right\n",
    "num_skips = 2 # How many times to reuse an input to generate a label\n",
    "num_sampled = 64 # Number of negative examples to sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a small chunk of Wikipedia articles collection\n",
    "url = 'http://mattmahoney.net/dc/text8.zip'\n",
    "data_path = 'text8.zip'\n",
    "if not os.path.exists(data_path):\n",
    "    print(\"Downloading the dataset... (It may take some time)\")\n",
    "    filename, _ = urllib.request.urlretrieve(url, data_path)\n",
    "    print(\"Done!\")\n",
    "# Unzip the dataset file. Text has already been processed\n",
    "with zipfile.ZipFile(data_path) as f:\n",
    "    text_words = f.read(f.namelist()[0]).lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dictionary and replace rare words with UNK token\n",
    "count = [('UNK', -1)]\n",
    "# Retrieve the most common words\n",
    "count.extend(collections.Counter(text_words).most_common(max_vocabulary_size - 1))\n",
    "# Remove samples with less than 'min_occurrence' occurrences\n",
    "for i in range(len(count) - 1, -1, -1):\n",
    "    if count[i][1] < min_occurrence:\n",
    "        count.pop(i)\n",
    "    else:\n",
    "        # The collection is ordered, so stop when 'min_occurrence' is reached\n",
    "        break\n",
    "# Compute the vocabulary size\n",
    "vocabulary_size = len(count)\n",
    "# Assign an id to each word\n",
    "word2id = dict()\n",
    "for i, (word, _)in enumerate(count):\n",
    "    word2id[word] = i\n",
    "\n",
    "data = list()\n",
    "unk_count = 0\n",
    "for word in text_words:\n",
    "    # Retrieve a word id, or assign it index 0 ('UNK') if not in dictionary\n",
    "    index = word2id.get(word, 0)\n",
    "    if index == 0:\n",
    "        unk_count += 1\n",
    "    data.append(index)\n",
    "count[0] = ('UNK', unk_count)\n",
    "id2word = dict(zip(word2id.values(), word2id.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47134"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "#next_batch函数\n",
    "\n",
    "def next_batch(batch_size,num_skips,skip_window):\n",
    "    global data_index\n",
    "    \n",
    "    span = 2*skip_window+1\n",
    "    \n",
    "    batch = np.ndarray(shape=(batch_size),dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size,1),dtype=np.int32)\n",
    "    \n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    \n",
    "    buffer.extend(data[data_index:data_index+span])\n",
    "    \n",
    "    data_index = data_index+span\n",
    "    \n",
    "    context_index = [w for w in range(span) if w!=skip_window]\n",
    "    \n",
    "    for i in range(batch_size//num_skips):\n",
    "        random_context = random.sample(context_index,num_skips)\n",
    "        \n",
    "        for j,word_id in enumerate(random_context):\n",
    "            batch[i*num_skips+j] = buffer[skip_window]\n",
    "            labels[i*num_skips+j,0] = buffer[word_id]\n",
    "            \n",
    "        if data_index==len(data):\n",
    "            buffer.extend(data[0:span])\n",
    "            data_index = span\n",
    "        \n",
    "        else:\n",
    "            buffer.append(data[data_index])\n",
    "            data_index+=1\n",
    "            \n",
    "    data_index = (data_index-span+len(data))%(len(data))\n",
    "    return batch,labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "#check\n",
    "batch,labels = next_batch(batch_size=batch_size,skip_window=skip_window,num_skips=num_skips)\n",
    "\n",
    "print(len(batch))\n",
    "print(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用tensorflow构建模型\n",
    "#占位符\n",
    "train_x = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "train_y = tf.placeholder(shape=[None,1],dtype=tf.int32)\n",
    "\n",
    "#获得词向量\n",
    "embedding = tf.Variable(tf.random_normal(shape=[vocabulary_size,embedding_size],dtype=tf.float32))\n",
    "\n",
    "X_embed = tf.nn.embedding_lookup(embedding,train_x)\n",
    "\n",
    "weights = tf.Variable(tf.random_normal(shape=[vocabulary_size,embedding_size],dtype=tf.float32))\n",
    "bias    = tf.Variable(tf.zeros(shape=[vocabulary_size],dtype=tf.float32))\n",
    "\n",
    "nce_loss = tf.reduce_mean(tf.nn.nce_loss(\n",
    "        weights = weights,\n",
    "        biases    = bias,\n",
    "        labels  = train_y,\n",
    "        inputs  = X_embed,\n",
    "        num_sampled=num_sampled,\n",
    "        num_classes = vocabulary_size\n",
    "))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(nce_loss)\n",
    "\n",
    "#compute 词向量consine距离，大致上判断效果\n",
    "X_norm = X_embed/(tf.sqrt(tf.reduce_sum(tf.square(X_embed),axis=1,keep_dims=True)))\n",
    "embedding_norm = embedding/(tf.sqrt(tf.reduce_sum(tf.square(embedding),axis=1,keep_dims=True)))\n",
    "dis_consine = tf.matmul(X_norm,embedding_norm,transpose_b=True)\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "x_test = np.array([word2id[w] for w in eval_words])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step_loss = 0.\n",
    "    for step in range(num_steps):\n",
    "        Batch,Labels = next_batch(batch_size=batch_size,skip_window=skip_window,num_skips=num_skips)\n",
    "        _,loss = sess.run([optimizer,nce_loss],feed_dict={train_x:Batch,train_y:Labels})\n",
    "        \n",
    "        step_loss+=(loss/display_step)\n",
    "        if (step+1)%(display_step)==0:\n",
    "            print(\"average step loss is %f\"%(step_loss))\n",
    "            step_loss = 0.\n",
    "        \n",
    "        if (step+1)%(eval_step)==0:\n",
    "            cos = sess.run(dis_consine,feed_dict={train_x:x_test})\n",
    "            print(\"Eval is starting.......\")\n",
    "            for i in range(len(x_test)):\n",
    "                \n",
    "                nearest_id = (-cos[i,:]).argsort()[1:8]\n",
    "                print(\"the nearst words of %s are\"%(eval_words[i]))\n",
    "                for word in nearest_id:\n",
    "                    nearest_word = \"\"\n",
    "                    nearest_word = \"%s,%s\"%(nearest_word,id2word[word])\n",
    "                print(nearest_word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
